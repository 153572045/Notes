


面试官好，我叫陈潇贤，目前在宝洁中国IT部做JAVA开发，在这边主要的工作有运维自动化平台，企微代理网关等项目的设计和开发，还有一些基础应用的迭代维
护，用到的技术包括了spring mysql redis等，然后之前是在京东推荐架构部做过推荐引擎开发，在那边的主要工作有 一些新推荐位的开发和维护，架构迁移，
算子和模块重构等。
------------------------------------------------------------------------------------------------------------------------
企微代理网关
之前我们这边的办公平台是微软的teams，后面为了方便才开通了企业微信， 相应的就会有一些应用想在企微上做一些自建应用，他们需要去调用企微的接口，调
用企微的接口需要一些令牌token,secret等信息，如果将这些东西都给了应用，他们可以去调任意接口，是一件安全性很低的行为
为了做更细力度的权限控制，我们就搞了一个中间的代理网关，这样应用想接入企微，需要通过网关，他们要调用哪个接口都需要我们
先在网关里面授权，而且我们也可以很方便的对这些调用行为去做监控，流量限制，因为企微本身也会限制流量。也提供了一些消息订阅服务

运维自动化平台
这是一个比较大的平台，我在里面主要是做了一个运维工单处理模块，运维工单就是我们平时要发布变更就需要提变更单，有事故就要提Incident事故单，然后由相
关的领导或者应用团队去处理，其实这些工作流的功能jira就能做，但是我们领导觉得jira用起来不够方便，所以这个模块是在jira的基础上做的一个封装，增加了
类似一键拉会、告警、协助Incident快速定级等 额外功能
------------------------------------------------------------------------------------------------------------------------
推荐引擎是京东推荐的一个业务逻辑承接系统，整个推荐流程是由这个引擎去驱动的，我简单说一下推荐的基本流程 一个基本的推荐流程pipline一般是包括了取数
商品召回，商品排序，还有最后的业务调控这四个环节，第一个环节取数就是去获取用户画像、用户行为等这些数据，拿到这些数据后就可以去做商品召回，商品召回
就是从不同的数据底池里面去捞数据，商品召回后进行汇总聚合，再由排序服务进行排序打分，最终返回待推荐的完整数据，再经过一系列业务调控，比如说过滤，这
就是推荐的一个基本流程。

我们的系统说得直白点，就是我们先从取数服务那边拿到画像等数据，再将这些数据作为输入去调用召回服务，召回服务给我们返回商品数据，我们再将商品数据作为输
入去调用排序服务，排序服务会给这些商品打分，我们拿到打完分的数据后会进行排序，业务调控，过滤等，最终再返回上游服务，总结起来我们的系统是一个通过组合
其他服务的一个业务调度系统，

树引擎

我们将整个推荐流程中的各个过程进行抽象并实现成一个个处理类(processor)，也叫处理算子，类似netty的pipeline中的一个处理器，每个算子实现相应的业务
逻辑，虽然整个推荐流程概括起来只有四个环节，但是每个环节里面包含大量逻辑，需要实现大量的处理算子去配合实现，而且不同环节的处理算子会交叉执行，比如说
一些处理算子之间如果有数据依赖关系，那么他们必须串行执行，如果没有数据依赖关系，就可以并发执行， 为什么我们要将一个流程拆分成那么多的处理算子去实现？
因为京东的推荐位有几百接近一千个，每个推荐位的具体逻辑都是不一样的，如果每个推荐位都按照各自的逻辑去做开发，那么维护起来是相当困难的，开发量也是巨大
所以我们将整个流程拆成一个个很小的处理算子去实现，每个处理算子以高度可复用的形式去设计，这样下来不同的推荐位的区别可能仅仅在于处理算子的编排，参数，策略不同

在对这些处理算子开发完成后，我们将一次完整的推荐流程用一个树或者是一个图来描述，用hocon这种数据格式在配置文件上进行描述，之后一个请求打到我们的系统上后
我们会根据请求里面的推荐位id找到相应的树配置文件，解析模块去解析这个文件，实际上就是去确定这次处理需要用到哪些处理算子，这些处理算子是怎么编排的，然
后去实例化相应的处理算子进行执行。

树引擎需要我们在配置文件上去指定各个处理算子的执行顺序，执行方式，哪些节点能并发执行，哪些不能， 一个推荐流程涉及到的处理算子有几十个，大推荐位有几百
个，这对于后期的维护工作还是比较困难的，而图仅仅需要配置哪些节点依赖于那些数据，每个节点会产出什么数据，不需要去关心他们的执行顺序以及执行方式，图引擎
会以数据依赖为驱动的执行方式，实现自动化最优并发执行的效果，也就是说当某个节点的依赖数据就绪，会立即被丢到线程里面去执行




遇到线上问题
遇到过一次比较棘手的线上问题， 有一次算法团队那边向我们反映我们组负责的一个推荐位的平均延迟从200ms涨到300ms，然后我就去排查，首先我通过监控找到延
迟上涨的具体时刻，然后去检查当天的代码，看看有没有异常的地方，看完代码后没有发现有异常，至少没看到很直观的异常代码，所以只能去回滚代码，然后将回滚后
的代码部署到测试机上进行测试，因为我们系统每天提交的代码量也比较杂，所以只能一步步进行回滚测试。
就这样回滚测试搞了大半天，还是没找到原因，回滚后延迟依然是很高，然后我就考虑有可能是其他服务引起的，不是我们这边系统的问题，就去看其他系统的监控，
一开始也没看出有什么异常的地方。后来从别的系统监控中观察到，一些系统在某一天的延迟突然平稳地下降，然后隔了好几天后又平稳地突然上涨，这个上涨的点刚好和
我们系统异常的时间点重合了，经过分析后才意识到前段时间是为了应付年货节，对机器进行了扩容，活动过后又进行了缩容，而这个异常可能被这个扩容效果给掩盖掉了，
等到扩容结束后才暴露出问题，所以我就把时间线回滚到扩容之前的代码，结果延迟确实也降下来了，后来通过再各个节点前后去打日志记录延迟，找到其中一个取数节点
的延迟忽高忽低，才找到问题所在。


高可用
在架构的设计上，我们几乎所有业务算子，就是业务类都是继承自同一个接口，同一个抽象类，就是说根据自己负责的功能去实现这个抽象
方法，然后这个方法被顶层抽象类中另一个方法所调用，调用的外层加了一层try/catch结构，catch根据不同类型的业务类会返回一些默认值，这一点尽可能减少了
整条数据链路因为某个业务类有bug抛出异常，导致整条链路出不了数据的情况。当然，因为我们系统的特点，在一些重要节点如果拿不到数据或者出现异常，也会最终
出不了结果，比如说在召回阶段，有可能网络超时原因或者那边服务不可用的原因导致我们这边拿不到数据，所以我们每次推荐都会用多个算子去调用多路召回服务，
即使其中某路出了问题，也有一些冷启的召回进行兜底，不会说出现空结果