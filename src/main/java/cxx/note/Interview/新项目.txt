


您好面试官，我叫陈潇贤，目前在宝洁中国IT部做Java开发，最近的一些的工作有运维自动化平台，企微代理网关等项目的设计和开发，还有一些基础应用的迭代维
护，用到的技术包括了spring mysql redis等，然后之前是在京东推荐架构部做过推荐引擎开发，在那边的主要工作有 一些新推荐位的开发和维护，架构迁移，
算子和模块重构。
------------------------------------------------------------------------------------------------------------------------
企微代理网关
之前我们这边的办公平台是微软的teams，后面开通了企业微信，相应的就有一些部门想在企业微信上做一些自建应用，做企微的自建应用可能会涉及到一些企
微官方接口的调用，要调这些接口的规则是先到企微管理后台拿到自建应用的唯一标识，用这个标识作为参数去调接口获取一个access_token，然后将token放到请求
头里面再去调具体的接口，所以之前只要有部门需要去调这些企微官方接口，我们都会直接把这些唯一标识字段给他们，他们自己去调，这也意味着他们有了这个标识后
可以去调任意接口，安全性是比较差的，毕竟有一些接口是可以查看或修改整个公司的人员信息，消费者信息，还有其他一些比较敏感的信息或操作。

所以我们为了做限制就在企微和应用之间做了一层代理网关，目的是对这些企微接口的调用权限做细粒化控制，也就是说现在应用想要调企微接口，都需要经过这层网关，
这样我们就可以在网关层做鉴权处理，他们想要调用某个接口前，需要先提单申请，审批通过后将权限映射关系记录到数据库里面，网关在收到请求后会先根据请求的信
息去数据库里面查找对应的权限，只有有权限的情况下才能进行透传，实现了按需分配具体接口调用权限的功能，而且我们也可以很方便通过这层网关去对这些调用行为
做监控，对一些敏感接口做纪录，方便后续追责。
同时也提供了一些消息订阅服务和分发功能

运维自动化平台
这是一个比较大的平台，我在里面主要是做了一个运维工单处理模块，运维工单就是我们平时要发布变更就需要提变更单，有事故就要提Incident事故单，然后由相
关的领导或者应用团队去处理，其实这些工作流的功能jira就能做(一些现成的项目管理工具就能做)，但是我们领导觉得jira用起来不够方便，所以这个模块是在
jira的基础上做了一些封装，增加了一键拉会、告警、协助Incident快速定级等额外功能
------------------------------------------------------------------------------------------------------------------------
推荐引擎就是推荐流程的一个驱动引擎，我简单说一下基本流程 一个基本的推荐流程一般包含了四个主要环节，即取数
商品召回，商品排序，还有业务调控，
1、第一个环节取数就是去获取用户的画像信息、行为信息等数据   个性化
2、有了用户信息后就可以做商品召回了，商品召回就是从海量的商品池里通过某种策略去捞取一小部分商品数据
3、商品召回后可能会有几百个或一两千个商品，然后就对这些商品进行排序打分
4、排完序后再经过一系列业务调控，比如说过滤，打散等
这就是推荐的一个基本流程。

我们系统在这个流程中的角色就是一个驱动引擎，就是对这四个环节进行精细化的控制，前面的三个环节，都会有一个专门算法工程团队在做，然后对我们提供服务，
我们的引擎系统作为推荐的入口服务，当接收到一个请求的时候，会先去调取数服务获取用户信息，再封装这些数据去调用召回服务，召回服务那边会根据我们的入参情况
给我们返回召回商品数据，我们再将这些商品结合其他参数去调排序服务，排序服务将这些商品进行排序打分后再给我们返回，最后我们会做一系列业务调控，再把最终的
结果返回给上游服务。


刚才我是从整体流程上做了个简述，实际上这四个环节内部的逻辑都是比较复杂的，而且京东主站的推荐位就有六七百个，虽然说这些推荐位在流程架构上，逻辑都
是由取数，召回，排序这几个环节构成，但是每个推荐位各个环节的逻辑细节都是不一样的，就拿商品召回来说，不同推荐位的召回底池是不一样的，召回策略可能也是
不一样的，有的是根据热门商品进行相关相似召回的，有的根据用户的点击行为或购买行为进行召回，有的根据长期画像标签或短期画像标签召回，业务调控环节更是多
样化，也就是说会有几百个不同的流程管道需要开发和维护，如果没有通用的架构支持，这种工作量肯定是很大的。
所以说对于我们系统而言，代码的复用性是非常重要的，另一个就是追求高性能低延迟了，我们需要保证一个推荐流程能在一
两百毫秒甚至几十毫秒内的时间内完成

高性能低延迟是怎么做的？ 整体的思想就是充分利用多线程去计算

如何解决复用性问题和低延迟
首先我们将整个流程中的每个环节(就是取数、召回、排序等环节)进行抽象并分割成许多子流程，在代码实现上每个子流程就实现成一个处理器，我们也叫处理
算子，就像netty pipline里面的Handler一样，每个处理算子承接一小部分业务逻辑，之后再将这些处理算子进行编排，让它们以多线程的
执行方式去配合完成一个大环节的逻辑。这样在复用性上能达到的效果就是，只要你的子流程处理算子抽象得够好，能充分考虑到各种场景的话，那开发一个新的推荐位
可能仅仅通过编排这些算子以及配置他们的参数就可以实现，代码开发量很低，也就是说不同的推荐位在理想的情况下可以共用一套代码。在低延迟上我们通过自研的树
或图引擎实现了对处理算子的高效编排  让各个处理算子能够以多线程的并行方式去执行

树引擎  -- 目的是对处理算子实现高效的编排，让这些算子能够以多线程的形式去高效执行

如果以简单的线性方式去对这些处理算子进行编排，让这些处理算子串行执行，那性能肯定达不到要求，一个推荐流程需要执行到的算子可能有几百个

将一次执行流程用树或图的形式来描述编排子，具体的实现我们是用一种类似json的数据格式hocon，在配置文件上先进行描述表达，因为树是由一系列节点构成的
嘛，配置的时候也是分别去配置每个节点的信息，从根节点开始配置，核心的配置信息包括了节点对应处理算子的类全限定名称、这个处理算子的一些属性值(实际执
行的时候会根据这个类全限定名称进行对象实例化并填充属性)，还包括节点的子节点信息，因为树是一种递归的数据结构，配置一个节点的时候需要配置它的子节点信息，
，这样一层层配置下去，最终完成整棵树的配置。
之后每一个请求进来，我们都会找到相应的配置文件，然后将这个配置文件解析成一个配置对象，然后由执行模块进行解析执行，执行也是从根节点处开始执行，
树上面的节点可以分为调度节点和业务节点，一个业务节点对应一个处理算子，承担具体的业务逻辑，而调度节点仅仅用来控制它子节点
的执行方式，比如说最常用的有串行调度和并行调度两种类型，当代码执行到一个业务节点的时候，就直接执行对应的处理算子逻辑，
当执行到一个串行调度节点的时候，就会以串行的方式去执行下面的子节点，如果是并行节点，就会以并行的方式去执行子节点

什么时候要串行？  如果算子之间有数据依赖关系  则需串行  如果没有  则可并行


树引擎需要我们在配置文件上去指定各个处理算子的执行顺序，执行方式，哪些节点能并发执行，哪些不能， 一个推荐流程涉及到的处理算子有几十个，大推荐位有几百
个，这对于后期的维护工作还是比较困难的，而图仅仅需要配置哪些节点依赖于那些数据，每个节点会产出什么数据，不需要去关心他们的执行顺序以及执行方式，图引擎
会以数据依赖为驱动的执行方式，实现自动化最优并发执行的效果，也就是说当某个节点的依赖数据就绪，会立即被丢到线程里面去执行


遇到线上问题
遇到过一次比较棘手的线上问题， 有一次算法团队那边向我们反映我们组负责的一个推荐位的tp99延迟从200ms涨到300ms，然后我就去排查，首先我通过监控找到延
迟上涨的具体时刻，然后去检查当天的代码，看看有没有异常的地方，看完代码后没有发现有异常，至少没看到很直观的异常代码，所以只能去回滚代码，然后将回滚后
的代码部署到测试机上进行测试，因为我们系统每天提交的代码量也比较杂，所以只能一步步进行回滚测试。
就这样回滚测试搞了大半天，还是没找到原因，回滚后延迟依然是很高，然后我就考虑有可能是其他服务引起的，不是我们这边系统的问题，就去看其他系统的监控，
一开始也没看出有什么异常的地方。后来从别的系统监控中观察到，一些系统在某一天的延迟突然平稳地下降，然后隔了好几天后又平稳地突然上涨，这个上涨的点刚好和
我们系统异常的时间点重合了，经过分析后才意识到前段时间是为了应付年货节，对机器进行了扩容，活动过后又进行了缩容，而这个异常可能被这个扩容效果给掩盖掉了，
等到扩容结束后才暴露出问题，所以我就把时间线回滚到扩容之前的代码，结果延迟确实也降下来了，后面针对那部分代码，通过阿尔萨斯不断去采样生成火焰图，找到
其中一个取数节点的延迟忽高忽低，才找到问题所在。 结果确实是其他服务的问题

宝洁异常
我讲一个最近遇到的线上内存溢出问题，我最近在做一个企微代理网关项目，其中有一个需求是要对某个接口的请求体和响应体进行记录，要在gateway上实现这个
功能其实很简单，就重写一个过滤器，过滤器里面有一些方法的参数就是requestbody或者responsebody,这些方法会被框架调用，就可以在这些方法里面去
实现我们要的逻辑，开发完后发布上线，一开始没出现什么问题，大概一周后，一些应用就跟我们反馈说我们的网关有内存溢出的异常。
我第一时间就去看了监控，发现内存确实飙得很高，但是堆里面的内存占用却不高，后面去容器里看了日志，发现抛出的异常是直接内存溢出的异常
因为做这个需求的时候比较急，我也是第一次接触网关，对这个框架的理解不够深，查阅了资料后，了解到这个框架底层的网络通信框架用的是netty，我重写的那些方法
很多都是基于缓存对象bytebuff去做操作，这些对象都是引用的堆外内存，所以在使用完之后要进行手动释放，返回对象池，否则这些堆外内存会一直积累，最终
溢出


高可用
1、集群的搭建上，因为我们的流量比较大，总共有三千多台机器在运行我们的系统，加机器也是提高可用性的一种策略，另一个是我们根据推荐位的重要程度和复杂程度
分配不同的资源，对于首页为你推荐这种大推荐位，我们专门用一个集群去处理，也采用专门的限流策略，因为执行一次复杂的推荐流程需要的资源更多，单机吞吐量差
距很大，以此来尽量保证重要推荐位的可用性和性能

2、在代码架构的设计上，我们对处理不同业务类型的处理算子进行分类，实现的时候继承自同一个接口，同一个抽象类，每一类算子的公共主流程在抽象类中以模板方法的方式
实现，核心功能再各自去实现，这样就可以在抽象类中很方便地去做一些统一的异常处理或兜底工作，尽可能减少了整条数据链路因为某个处理算子有异常或者拿不到数据导致
整条链路出不了数据的情况。当然，因为我们系统的特点，在一些重要节点如果拿不到数据或者出现异常，也会最终出不了结果，比如说在召回阶段，有可能网络超时原因
或者那边服务不可用的原因导致我们这边拿不到数据，所以我们每次推荐都会用多个算子去并发调用多路召回服务，即使其中某路出了问题，也有一些冷启的召回进行兜底，
不会说出现空结果。

排序分片  分摊压力，风险

3、有降级 限流 和熔断等策略来保证我们系统的稳定性和安全

4、兜底


架构迁移

排序算法
          平均复杂度       最好           最差
快排          nlogn       nlogn         n*n       不稳定
堆排          nlogn       nlogn         nlogn     不稳定
归并          nlogn       nlogn         nlogn     稳定
