

面试官您好，我叫陈潇贤，目前在宝洁中国IT部做后端开发，平时主要是做平台相关的服务，最近做的项目有运维自动化平台(通知中心)，企微平台(包括聊天记
录审计，代理网关，人员管理等)，在此之前是在京东推荐架构做过推荐引擎开发，在那边的主要工作有 一些新推荐位的开发和维护，架构重构，算子迁移。
这是我的一个基本情况  谢谢


------------------------------------------------------------------------------------------------------------------------
企微平台
之前我们这边的办公平台是微软的teams，后面开通了企业微信作为我们平时的工作交流平台，相应的就需要去做一些企微开发，比如说人员通讯录，消费者管理。

为了让企微的使用和开发更规范更安全，我们就需要去做一些基础组件，其中我负责的就有这个代理网关和审计系统

企微代理网关
做企微开发是需要去调企微的一些官方API的，调这些API需要做鉴权，具体方式是先到公司的企微管理后台拿到应用的唯一标识，再用这个标识通过接口获取一
个access_token，最后才能去调具体的接口，所以之前只要有部门要去调这些企微API，我们都会直接把这些唯一标识给他们，他们自己去调，这也意味着他们有了
这个标识后可以去调任意接口，安全性是比较差的，毕竟有一些接口是可以修改公司的人员信息，查看消费者信息等。


所以我们为了做限制就在企微和应用之间做了一层代理网关，目的是对这些企微接口的调用权限做细粒化控制，也就是说现在应用想要调企微接口，都需要经过这层网关，
这样我们就可以在网关层做鉴权处理，他们想要调用某个接口前，需要先提单申请，审批通过后将权限映射关系记录到数据库里面，网关在收到请求后会先检查调用方
是否有相应接口的调用权限，有权限则透传，实现了按需分配接口调用权限的功能，而且我们也可以很方便通过这层网关去对这些调用行为做监控，对一些敏感接
口做纪录，方便后续追责。
同时也提供了一些消息订阅服务和分发功能


运维自动化平台
这是一个比较大的平台，我在里面主要是做了运维工单处理、通知中心和数据分析等模块，运维工单就是我们平时要发布变更就需要提变更单，有事故就要提Incident
事故单，然后由相关的领导或者应用团队去处理，也有一些工单是自动化处理，其实这些工作流的功能（一些现成的项目管理工具就能做 jira）就能做，但是我们领导
觉得jira用起来不够方便，所以这个模块是在jira的基础上做了一些二次开发，增加了一键拉会、告警、数据分析等额外功能。


通知中心
通知中心就是一个通知的代理组件，如果没有这个服务，各个应用想要使用通知功能，就需要自己去对接各种渠道的通知服务(比如邮件、teams、企微、短信)，这个
过程是很繁琐的，而我们做的就是在通知中心内统一接入这些通知渠道，然后对外暴漏统一的API供应用直接使用，屏蔽了各种通知渠道接入时繁琐复杂的流程，也提供
了消息模版管理功能，来加快通知服务的接入和发送效率
内部也做了一些重试机制，来保证通知的可靠性


宝洁异常
前段时间遇到过一个线上内存溢出的问题，这个问题发生在网关项目，有个需求需要对某个接口的请求体和响应体做一些记录，要在gateway上实现这个
功能其实很简单，就重写一个过滤器，过滤器里面有一些方法的参数就是requestbody或者responsebody,这些方法会被框架调用，我们可以在这些方法里面去
实现我们要的逻辑，开发完后发布上线，一开始没出现什么问题，大概一周后，一些应用就跟我们反馈说我们的网关有内存溢出的异常。
我第一时间就去看了监控，发现容器的内存确实飙得很高，但是堆里面的内存占用却不高，后面去容器里看了日志，发现抛出的异常是直接内存溢出的异常
因为做这个需求的时候比较急，对webflux的理解也不够深，经过排查后，发现我重写的那些方法
有一些动作都是基于缓存对象bytebuff去做操作，这些对象都是引用的堆外内存(因为gateway底层的网络通信架构用的是netty)，所以在使用完之后要进行手动
释放，返回对象池，否则这些堆外内存会一直积累，最终溢出


------------------------------------------------------------------------------------------------------------------------
面试官您好，我叫陈潇贤，目前在宝洁中国IT部做后端开发，最近的工作主要是在做一些平台相关的服务项目，比如运维自动化平台(通知中心)，企微平台(包括聊天记
录审计，代理网关，人员管理等)，在此之前是在京东推荐架构做过推荐引擎开发，当时在那边的主要工作有 一些新推荐位的开发和维护，架构重构，算子迁移。
这是我的一个基本情况  谢谢
------------------------------------------------------------------------------------------------------------------------
推荐系统中的调度引擎，它的作用是将召回，粗排，精排和重排等流程串起来 构成一条完整的推荐链路

像推荐这种比较复杂的任务流的处理场景，一般会用任务处理管道来做实现，一条处理管道由各个节点构成，请求进入管道后会依次被各个节点处理，这种模式的好
处在于节点之间的解耦度比较高，管道可以进行插拔式地拓展，而且代码具有可用复用性，很多通用节点可以被不同推荐位复用。 而树推荐引擎或图推荐引擎，就是以
树或图的形式来组织处理节点的调度系统，它们可以以更加灵活、高效的方式去编排各个节点，不仅可以控制节点之间的执行顺序，还可以控制它们的执行方式，比如并
发执行或串行执行，从而提高整条任务管道的并发度，让计算快速完成，达到低延迟的效果

将业务从引擎系统中剥离，有利于聚焦引擎系统的功能和业务梳理
------------------------------------------------------------------------------------------------------------------------
工作任务

1、每个人都会负责部分推荐位的开发和维护，我主要负责主站商详、七鲜和泰国站一些推荐位

2、架构重构，算子迁移，我在职那会刚好在做系统重构，就是树引擎到图引擎的重构，这个图引擎是我们那的架构师提出来的，他是C++背景，就给我们提供了一些
C++相关的资料，我们在这个基础上用java把系统搭起来，我当时主要负责系统一些基础设施的重构，比如图结构，图对象生命周期管理(也就是池化)，rpc,因为
我们旧系统很多调用方式还是使用Http， 还有就是算子迁移了，就是将业务算子从旧系统迁到新系统，根据新的架构特性去重构，我主要负责召回、过滤和部分取数
模块的算子迁移，因为新系统的数据流处理方式和旧系统都有比较大的区别，而且旧系统算子冗余严重，在迁移的时候要考虑各种复用性，最大化去降低算子的种类。
迁移后还要做diff，保证同一个请求走新系统和旧系统都能得到一样的结果。

算子重构概括起来就是 1、梳理业务  2、对业务进行分类重抽象，为了降低算子的冗余度，对于旧系统的算子
我都会尽量去做合并，比较常用的方式是通过策略模式去封装各种场景算法，这样可以减少不少冗余算子，还有更多会参照新系统的特性去重构吧，比如过滤模块，在
树引擎上是每个过滤逻辑都是一个单独的算子节点，因为每个推荐位基本都有十几二十个过滤逻辑，处理管道上的节点会比较膨胀，在新系统上，我只写了一个过滤算
子，具体的过滤逻辑再封装到非算子的业务类中，然后通过属性聚合的方式关联起来，初始化过滤算子的时候根据具体的配置需要去加载，这样会大大减少了一个处
理管道上的节点数，对开发和维护效率都有提升

3、优化
启动优化， 序列化优化，数据结构调整           IO密集型系统  大数据量场景 使用Thrift序列化方案
------------------------------------------------------------------------------------------------------------------------
高可用

1、集群的搭建上，因为我们的流量比较大，总共有三千多台机器在运行我们的系统，机器多也是高可用的一种策略，另一个是我们分集群处理流量，对一些重要程度和
复杂程度比较高的推荐位，会用专门的集群处理，分配更多的资源，尽量保证这些重要推荐位的可用性和性能。
也就是说一些重要推荐位的请求，会被路由到特定的集群。


2、有降级 限流 和熔断等策略来保证我们系统的稳定性和安全


3、我们对可用率的统计是根据调用结果能否出数，所以基本每个推荐位都会有一些冷启召回来避免召回不到数据，即使真出不了数据，我们的上游服务会给我们做弱个
性化兜底


4、在代码架构的设计上，我们每一类处理算子基本都会在相应的抽象类中，通过模版方法做了统一的异常处理和兜底工作，去尽可能减少因为某个处理算子有异常而导致
整条链路出不了数据的情况。


调其他服务的一些策略   排序分片  分摊压力，风险
------------------------------------------------------------------------------------------------------------------------



排序算法
          平均复杂度       最好           最差
快排          nlogn       nlogn         n*n       不稳定
堆排          nlogn       nlogn         nlogn     不稳定
归并          nlogn       nlogn         nlogn     稳定
