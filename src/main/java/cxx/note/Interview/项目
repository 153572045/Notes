梳理一下项目（角色，难点，收获，技术点，项目细节）


微服务(Microservices) 是一种软件架构风格，将复杂应用程序的多个服务模块进行拆分，拆分成多个自治的小型服务，彼此之间用语言无关的API集及进行通讯。
优势：
更容易维护，也能更加有效地利用计算资源。

我们这边的系统是京东推荐的一个业务逻辑承接系统，抛开细节来讲，一个推荐流程一般是包括了用户画像、用户行为取数，商品召回，商品排序，还有过滤等环节
而我们的系统就是将这些环节集成起来的一个逻辑调度系统，具体的实现是通过RPC去调用以上这些推荐服务，并做数据组合来实现一个完整的推荐过程。
也就是说用户画像取数，商品召回，排序这些环节分别都有一个团队在做，并向我们提供服务接口，当然我们也是对外提供rpc接口，由我们的上游服务来调。
当一个请求打到我们系统后，我们首先会去调用画像服务，从那边拿到用户的数据，然后再将这些数据作为输入去调用召回服务，召回服务就会给我们返回商品数据，再
将返回的商品数据作为输入去调用排序服务，排序服务就会对我们的商品做一个排序再返回给我们，最后我们再对这些数据做一系列过滤和截断，再返回给我们的上
游服务。 所以总结起来，我们的系统是一个通过组合其他服务的一个业务调度系统，所以也叫推荐引擎。
是一个IO密集型的系统

2、树引擎是什么

我们将一次请求的执行逻辑(也就是一个完整推荐流程的执行过程)用一棵树来描述，这种树是用hocon这种数据格式描绘在一个config配置文件中的，每个推荐位的具体
推荐逻辑不同，所以需要用不同的配置文件来配置对应的树，然后每个推荐位都对应着一个配置文件，当一个请求打到我们系统的时候，我们会根据请求中的
pid（推荐位id）去获取一个对应的树配置文件，然后我们代码会对这个配置文件进行解析，转成一个config对象(之后这个config对象就携带了这一整颗树上面的信息)
然后代码中的调度模块会根据这个config去执行逻辑。
然后树的执行过程是这样子的：
树中的节点分为业务节点和调度节点两种类型，业务节点对应着具体的业务对象，而调度节点起到对这些业务节点的编排作用，比如说我们有两种比较常见的调度节点，
分别是并行节点和串行节点，我们的代码会从树的根节点开始解析，当解析到的节点是串行节点的时候，就会以串行的方式去执行这个调度节点的子节点，当解析到一个
并行节点的时候，就会以并行的方式去执行它的子节点，解析到一个业务节点的时候，就会去执行这个业务节点的业务方法。
具体在执行到某个业务节点的时候，会根据config对象中相应的数据用反射注入到具体的业务对象中，这样就可以起到配置化开发的效果，大大地提高代码的复用效率和
整体的工程迭代效率

优点：树调度引擎让我们可以去编排各个业务节点的调度顺序和调度形式，哪些节点可以并发执行，哪些节点因为具有依赖关系只能串行调度，在没有业务逻辑改变的
情况下，这些只需要改变配置文件的配置就可以进行调整。
缺点：因为我们不同的推荐位的复杂程度不一样，当推荐位比较复杂的时候，一棵树的节点会有上百个，人工编排节点的工作量就会变得比较大，而且出错的可能性会比较
大，比如说有些节点可以并发执行，但是你配置成串行执行了，这样虽然结果不会改变但是影响了并发效率。甚至有些节点因为具有数据依赖关系应该串行执行被你配置成
并行执行，那么结果就会出问题。因此我们将调度引擎升级到图引擎

3、图引擎是什么
将手动编排节点执行顺序的方式转换成为以数据依赖为驱动的执行方式，在图引擎中所有节点都是业务节点，没有调度节点，每个节点只需要关心自己依赖的数据和产出的
数据，我们的代码也是会去解析hocon配置文件然后产生一个图对象，构建图对象的过程中会根据文件的配置信息确立各个节点之间的依赖关系，并在运行过程中，让数据
依赖就绪的节点尽早运行，达到无需我们手动调整，就可以实现自动化最优并发执行的效果。
具体的实现是这样的，代码会先从图的出口节点，也就是产出最终结果的那个节点处从后往前进行遍历，这个过程会去获取各个节点的依赖关系并记录到一些map中，
这个过程也称为激活，激活完后从图的入口节点，也就是第一个节点开始运行，第一个节点的依赖数据其实就是一个请求，我们系统接到一个请求时会把这份数据送到第
一个节点，那么它的依赖数据就绪了，就可以直接运行了， 节点运行完后会产出数据，这个时候它们会去搜索激活阶段的依赖关系，找到依赖于这个产出数据的节点，然
后把它的依赖数量做减一操作，如果减一操作后该节点的依赖数为0，那么直接将该节点放到线程池中运行，这样就实现了依赖数据就绪的节点可以立即执行。

迁移过程的描述：重构
从树引擎升级到图引擎的过程是这样的，首先这个图引擎的架构是我们那边的架构师设计的，然后由几个高T把基本的架构给搭起来，这个过程是相当于重新写了一个框架，
有了这个框架之后，我们才能将旧系统中具体的业务算子(就是一些功能类)根据新架构的特性在新系统上面去重新实现，因为这个新架构有比较大的改变，特别是数据传
输这一块和旧架构是完全不同的，所以在做功能迁移的时候不是简单地把代码复制过去，是需要重新梳理这个业务逻辑，然后根据新的架构特性去做重构，这个过程也是
比较耗时的，所以在迁移早期，有些同事迫于压力，为了加快项目的进展，还是或多或少会去直接复制旧系统的代码，因为也确实存在一些可以直接复制的模块，虽然
在功能上没啥问题，但是对以后的复用或扩展可能就不是很友好了，而且这种直接复制的方法不仅写的时候省事，而且在进行功能压测验证的时候出问题的概率也会低很多
，因为毕竟这些代码是之间经过验证过的，也有一些人将一些模块写死，不考虑复用性，也各自开发各自的东西，导致系统的冗余代码比较严重，所以我后面的工作就去对
这类代码进行梳理，重新抽象和实现，主要负责的模块是一些取数模块和召回模块。

diff工具的开发
因为迁移系统，在迁移完某个功能后，肯定是需要在新旧系统上做一个对比分析，我们最终的目标是要实现同一个用户的同一个请求，打到新旧系
统上，这两者的输出必须保证一致，如果不一致，我们就要去分析是哪些环节导致的不一致，不一致的原因是什么。因为我们的系统是一个逻辑调度系统，涉及到的环节非
常多，所以这个对比分析工作就非常繁琐，因为我们必须保证每一个环节的数据都完全一致，才能实现最终结果的一致，比如对其中某个服务的请求protobuf必须是一
致，那边服务的返回结果也必须一致，因为环节众多，而且每个环节涉及到的数据也非常多，单靠人工去排查效率就极低。所以就需要diff工具来支持

diff工具的具体实现其实就是在各个环节前后对请求的proto和服务返回的proto做一些日志记录，然后再把这些日志记录提交到缓存系统，再由QA那边去拿数据，之后
开发一些对比脚本去做对比，最后将不一致的点进行记录，供迁移人员去查看分析。

开发diff工具遇到的问题
diff工具的开发在技术上倒没遇到什么比较难解决的问题
技术上也有一个吧，这个diff工具一开始是将这些日志信息返回到一个我们平时做debug测试的平台前端，但是随着数据量的增加，到后面因为数据太多，导致网络延迟
超过我们这个平台的一个延时时长上线，返回空结果，所以是无法在这个平台上去做debug对比，后来我就把这些数据存到jimdb，然后让QA去拿，避免了将大量数据
返回前端的情况。
其他问题更多的是在做迁移的时候比较麻烦，因为我们在做迁移的时候是以某个推荐位为单位的，迁移一个推荐位会拉一个小组，然后每个小组成员会按模块来进行分工，
比如说有的同事负责取数模块，有的负责召回，有的负责排序模块，其中召回模块是在取数之后嘛，也就是说依赖于取数模块，因为你只有先保证取数模块功能对齐了，
才能去对齐召回模块，这种依赖关系就很麻烦，后来我们就采用了mock的方式去做迁移对齐，就是说负责召回模块迁移的同事自己去mock取数模块，保证同一个请求打
到测试机，两个系统的取数结果是一致的。具体的做法其实就是用缓存来解决，我们会搭一个mock服务，然后测试机去调取数服务的时候是打到这个mock服务的机器上，
对于同一个请求，我们以请求为key,如果请求的Key存在，则直接从缓存返回，如果不存在，则去调正常服务并做缓存，这样我们就能保证各个阶段互不依赖，提高效率。

6、你们的系统是如何做到高可用高并发的

首先从高可用这一块来讲吧，在架构的设计上，我们几乎所有业务算子，就是业务类都是继承自同一个接口，同一个抽象类，就是说根据自己负责的功能去实现这个抽象
方法，然后这个方法被顶层抽象类中另一个方法所调用，调用的外层加了一层try/catch结构，catch根据不同类型的业务类会返回一些默认值，这一点尽可能减少了
整条数据链路因为某个业务类有bug抛出异常，导致整条链路出不了数据的情况。当然，因为我们系统的特点，在一些重要节点如果拿不到数据或者出现异常，也会最终
出不了结果，比如说在召回阶段，有可能网络超时原因或者那边服务不可用的原因导致我们这边拿不到数据，所以我们每次推荐都会用多个算子去调用多路召回服务，
即使其中某路出了问题，也有一些冷启的召回进行兜底，不会说出现空结果。
在比较高峰的时候，我们会做一些降级处理，比如说降低模型的复杂度，减少召回数量等一些降级措施，即使因为某个环节出问题最终出不了结果，我们的上游服务会给我
们做若个性化的兜底，他们再出不了的话前端那边会做一些缓存兜底。
当然到了最后还是需要做限流

高并发这一块，因为我们有两套架构嘛，旧的树引擎，还有新架构图引擎，之前使用树引擎我们可以手动去编排各个节点的执行顺序和执行方式，一些彼此之间没有数据
依赖关系的节点可以采用并发的方式去执行，有依赖关系的节点就采用串行执行，当然毕竟是需要手工去控制，在一些复杂推荐位中这些树节点有上百个，编排起来还是比
较麻烦，也有可能出错，所以在图引擎换成了以数据依赖驱动的方式去执行，每个图节点仅关注自己的依赖数据和产出数据，如果某个节点的依赖数据就绪，那么它可以立刻
被丢入线程池执行，若它执行完毕后产出数据，就会去激活依赖于它的节点，让下一个节点立即去执行。这样可以达到无需我们手动调整，就可以实现自动化最优并发执行
的效果。