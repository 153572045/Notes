

面试官您好，我叫陈潇贤，目前在宝洁中国IT部做后端开发，平时主要是做平台相关的服务，最近做的几个项目有运维自动化平台，企微平台(包括聊天记录审计，代理
网关，人员管理等)，在此之前是在京东推荐架构做过推荐引擎开发，在那边的主要工作有 一些新推荐位的开发和维护，架构迁移，算子和模块重构。
这是我的一个基本情况  谢谢

用到的技术包括了spring mysql redis等
------------------------------------------------------------------------------------------------------------------------
企微代理网关
之前我们这边的办公平台是微软的teams，后面开通了企业微信，相应的就有一些部门想在企业微信上做一些自建应用，做企微的自建应用会涉及到一些企
微官方接口的调用，要调这些接口的规则是先到企微管理后台拿到自建应用的唯一标识，用这个标识作为参数去调接口获取一个access_token，然后将token放到请求
头里面再去调具体的接口，所以之前只要有部门需要去调这些企微官方接口，我们都会直接把这些唯一标识字段给他们，他们自己去调，这也意味着他们有了这个标识后
可以去调任意接口，安全性是比较差的，毕竟有一些接口是可以查看或修改整个公司的人员信息，消费者信息，还有其他一些比较敏感的信息或操作。

所以我们为了做限制就在企微和应用之间做了一层代理网关，目的是对这些企微接口的调用权限做细粒化控制，也就是说现在应用想要调企微接口，都需要经过这层网关，
这样我们就可以在网关层做鉴权处理，他们想要调用某个接口前，需要先提单申请，审批通过后将权限映射关系记录到数据库里面，网关在收到请求后会先根据请求的信
息去数据库里面查找对应的权限，只有有权限的情况下才能进行透传，实现了按需分配具体接口调用权限的功能，而且我们也可以很方便通过这层网关去对这些调用行为
做监控，对一些敏感接口做纪录，方便后续追责。
同时也提供了一些消息订阅服务和分发功能

运维自动化平台
这是一个比较大的平台，我在里面主要是做了一个运维工单处理模块，运维工单就是我们平时要发布变更就需要提变更单，有事故就要提Incident事故单，然后由相
关的领导或者应用团队去处理，其实这些工作流的功能jira就能做(一些现成的项目管理工具就能做)，但是我们领导觉得jira用起来不够方便，所以这个模块是在
jira的基础上做了一些封装，增加了一键拉会、告警、数据分析等额外功能
------------------------------------------------------------------------------------------------------------------------






推荐引擎就是推荐流程的一个驱动引擎
一个推荐流程一般包括了召回，粗排，精排和重排等环节(重排也就是业务调控)， 因为召回需要依赖一些数据，所以召回之前实际还有一个取数环节

我们系统在这个流程中的角色就是一个驱动引擎，就是对这些环节进行精细化的控制，

------------------------------------------------------------------------------------------------------------------------

如何解决复用性问题和低延迟
首先我们将整个流程中的每个环节(就是取数、召回、排序等环节)进行抽象并分割成许多子流程，在代码实现上每个子流程就实现成一个处理器，我们也叫处理
算子，每个处理算子承接一小部分业务逻辑，之后再将这些处理算子进行编排，以树或图的结构来实现高效编排，让它们
以多线程的执行方式去配合完成一个大环节的逻辑。这样在复用性上能达到的效果就是，只要你的子流程处理算子抽象得够好，能充分考虑到各种场景的话，那开发一个
新的推荐位可能仅仅通过编排这些算子以及配置他们的参数就可以实现，代码开发量很低，也就是说不同的推荐位在理想的情况下可以共用一套代码，区别只在于这些
处理算子的编排顺序和内部属性不同。
在低延迟上我们通过自研的树或图引擎实现了对处理算子的高效编排  让各个处理算子能够以多线程的并行方式去执行

树引擎  目的-- 实现处理算子的高效编排，让这些算子能够以多线程的形式去高效执行

相对于树或图，如果以简单的线性方式去组织这些处理算子，让这些处理算子串行执行，那性能肯定达不到要求，一个推荐流程下来，需要执行到的算子可能有几百个

树引擎就是以树形的结构来组织编排处理链路，具体的实现我们是用一种类似json的数据格式hocon，在配置文件上先进行描述表达，因为树是由一系列节点构成的
嘛，配置的时候也是分别去配置每个节点的信息，配置的信息包括了节点对应处理算子的类名称、这个处理算子的一些属性值(实际执
行的时候会根据类名进行对象实例化并填充属性)，因为树是一种递归的数据结构，配置一个节点的时候需要配置它的子节点信息，
，这样一层层配置下去，最终完成整棵树的配置。
之后每一个请求进来，我们都会找到相应的配置文件，然后将这个配置文件解析成一个配置对象，然后由执行模块进行解析执行，执行的时候是从根节点处开始执行，
树上面的节点可以分为调度节点和业务节点，一个业务节点对应一个处理算子，承担具体的业务逻辑，而调度节点仅仅用来控制它子节点
的执行方式，比如说最常用的有串行调度和并行调度两种类型，当代码执行到一个业务节点的时候，就直接执行对应的处理算子逻辑，
当执行到一个串行调度节点的时候，就会以串行的方式去执行下面的子节点，如果是并行节点，就会以并行的方式去执行子节点



遇到线上问题
遇到过一次比较棘手的线上问题， 有一次算法团队那边向我们反映我们组负责的一个推荐位的tp99延迟从200ms涨到300ms，然后我就去排查，首先我通过监控找到延
迟上涨的具体时刻，然后去检查当天的代码，看看有没有异常的地方，看完代码后没有发现有异常，至少没看到很直观的异常代码，所以只能去回滚代码，然后将回滚后
的代码部署到测试机上进行测试，因为我们系统每天提交的代码量也比较杂，所以只能一步步进行回滚测试。
就这样回滚测试搞了大半天，还是没找到原因，回滚后延迟依然是很高，然后我就考虑有可能是其他服务引起的，不是我们这边系统的问题，就去看其他系统的监控，
一开始也没看出有什么异常的地方。后来从别的系统监控中观察到，一些系统在某一天的延迟突然平稳地下降，然后隔了好几天后又平稳地突然上涨，这个上涨的点刚好和
我们系统异常的时间点重合了，经过分析后才意识到前段时间是为了应付年货节，对机器进行了扩容，活动过后又进行了缩容，而这个异常可能被这个扩容效果给掩盖掉了，
等到扩容结束后才暴露出问题，所以我就把时间线回滚到扩容之前的代码，结果延迟确实也降下来了，后面针对那部分代码，通过阿尔萨斯不断去采样生成火焰图，找到
其中一个取数节点的延迟忽高忽低，才找到问题所在。 结果确实是其他服务的问题

宝洁异常
我讲一个最近遇到的线上内存溢出问题，我最近在做一个企微代理网关项目，其中有一个需求是要对某个接口的请求体和响应体进行记录，要在gateway上实现这个
功能其实很简单，就重写一个过滤器，过滤器里面有一些方法的参数就是requestbody或者responsebody,这些方法会被框架调用，我们可以在这些方法里面去
实现我们要的逻辑，开发完后发布上线，一开始没出现什么问题，大概一周后，一些应用就跟我们反馈说我们的网关有内存溢出的异常。
我第一时间就去看了监控，发现容器的内存确实飙得很高，但是堆里面的内存占用却不高，后面去容器里看了日志，发现抛出的异常是直接内存溢出的异常
因为做这个需求的时候比较急，我也是第一次接触网关，对这个框架的理解不够深，查阅了资料后，了解到这个框架底层的网络通信框架用的是netty，我重写的那些方法
有一些动作都是基于缓存对象bytebuff去做操作，这些对象都是引用的堆外内存，所以在使用完之后要进行手动释放，返回对象池，否则这些堆外内存会一直积累，最终
溢出


高可用
1、集群的搭建上，因为我们的流量比较大，总共有三千多台机器在运行我们的系统，加机器也是提高可用性的一种策略，另一个是我们根据推荐位的重要程度和复杂程度
分配不同的资源，对于首页为你推荐这种大推荐位，我们专门用一个集群去处理，也采用专门的限流策略，因为执行一次复杂的推荐流程需要的资源更多，单机吞吐量差
距较大，以此来尽量保证重要推荐位的可用性和性能

2、在代码架构的设计上，我们对处理不同业务类型的处理算子进行分类，实现的时候继承自同一个接口，同一个抽象类，每一类算子的公共主流程在抽象类中以模板方法的方式
实现，核心功能再各自去实现，这样就可以在抽象类中很方便地去做一些统一的异常处理或兜底工作，尽可能减少了整条数据链路因为某个处理算子有异常或者拿不到数据导致
整条链路出不了数据的情况。当然，因为我们系统的特点，在一些重要节点如果拿不到数据或者出现异常，也会最终出不了结果，比如说在召回阶段，有可能网络超时原因
或者那边服务不可用的原因导致我们这边拿不到数据，所以我们每次推荐都会用多个算子去并发调用多路召回服务，即使其中某路出了问题，也有一些冷启的召回进行兜底，
不会说出现空结果。

调其他服务的一些策略   排序分片  分摊压力，风险

3、有降级 限流 和熔断等策略来保证我们系统的稳定性和安全

4、兜底


架构迁移

排序算法
          平均复杂度       最好           最差
快排          nlogn       nlogn         n*n       不稳定
堆排          nlogn       nlogn         nlogn     不稳定
归并          nlogn       nlogn         nlogn     稳定
